# Final Week: Report 13 #
## Week of 12/02-12/05
### 1. Reflections
In the second week, our focus transitioned to integrating the machine learning models with the Unity interface and exploring advanced features to enhance the user experience. A significant portion of our efforts was dedicated to implementing Google's Face Landmark Detection as a tool to reconstruct a digital twin of the user. This involved connecting the facial expression data from our highly accurate CNN model with Unity to create an animated character that mirrors the user's expressions in real-time.

Using Google's Face Landmark Detection, we were able to accurately map facial landmarks and translate them into corresponding animations within Unity. This integration allows the digital twin to perform facial expressions that reflect the user's inputs, providing immediate visual feedback and making the rehabilitation process more engaging and interactive.

Figure 3: Animated character in Unity responding to user’s facial expression of happiness.

Figure 4: Animated character in Unity responding to user’s facial expression of surprise.

Additionally, we integrated the ChatGPT API into Unity to develop an adaptive storytelling feature. This feature generates dynamic narratives that guide users through their rehabilitation exercises, making the experience more enjoyable and motivating. By leveraging ChatGPT, the system can create personalized storylines that adapt based on the user's progress and interactions, ensuring that the rehabilitation process remains both effective and engaging.

Despite these advancements, we encountered challenges in fine-tuning the digital twin's responsiveness and ensuring that the storytelling narratives remain contextually relevant and engaging. These issues highlighted the need for ongoing refinement and user feedback to optimize both the digital twin and the interactive storytelling components.







<div align=center><img width="500" alt="Learning Rhino" src="assets/Week12-13/Diagram 3.png"></div>

<br><br>




### 2. Speculations

Future developments for FaceStory include enhancing the digital twin's realism and responsiveness by incorporating more sophisticated facial landmark detection algorithms and refining the animation parameters within Unity. We also plan to expand the storytelling capabilities to include a wider range of narratives and adaptive difficulty levels, ensuring that the rehabilitation exercises remain challenging and engaging as users progress.

A pertinent industry trend is the increasing use of AI-driven virtual assistants in healthcare applications, which aligns perfectly with our goal of creating an interactive and supportive rehabilitation tool. By leveraging these advancements, FaceStory can offer personalized therapy experiences that adapt to each user's unique needs and progress, ultimately improving rehabilitation outcomes.

Figure 5: AI-driven personalized therapy programs enhancing patient engagement and outcomes.

Furthermore, we intend to explore the integration of real-time user feedback mechanisms to dynamically adjust the difficulty and engagement level of the rehabilitation exercises. This will involve incorporating additional sensors and refining our machine learning models to better understand and respond to user needs in real-time.

By continuously iterating on our design and incorporating cutting-edge technologies, FaceStory aims to revolutionize facial rehabilitation, making it more accessible, efficient, and enjoyable for patients with facial mobility challenges.
